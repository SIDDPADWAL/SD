{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572f3104-ca03-47e8-9f47-28c1a48c1f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c5d018-2b07-431f-ac24-77a9a37c022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90012296-e3af-41dd-b24b-4591a20df0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello everyone! Welcome to my blog post on Medium. We are studying Natural Language Processing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28dfb60b-cf5e-4440-b279-146de75dd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "563d37f6-c6c9-47b7-a06f-4e21ccd52d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello everyone! Welcome to my blog post on Medium. We are studying Natural Language Processing.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ff5f1f-3d0c-4db4-887f-213ebb4bbade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone!', 'Welcome to my blog post on Medium.', 'We are studying Natural Language Processing.']\n"
     ]
    }
   ],
   "source": [
    "tokens_sents = nltk.sent_tokenize(text)\n",
    "print(tokens_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19f8d2fe-eb65-4ddb-8462-42b0f8477258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '!', 'Welcome', 'to', 'my', 'blog', 'post', 'on', 'Medium', '.', 'We', 'are', 'studying', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_words = nltk.word_tokenize(text)\n",
    "print(tokens_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c195e4-042d-42b2-b734-bb38cd51a8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'civil'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "word = (\"civilization\")\n",
    "ps.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4639366-6e24-49b8-9165-c3045921a527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'civil'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language = \"english\")\n",
    "word = \"civilization\"\n",
    "stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81f220ed-4713-42df-a16c-f1d55b880ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c55f2f3e-c37f-4c67-8a41-60228e67aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker\n",
      "beech\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"workers\"))\n",
    "print(lemmatizer.lemmatize(\"beeches\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2c17e0b-3c0f-4d2c-b647-ccd9186a0d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', '’', 's', 'lemmatize', 'a', 'simple', 'sentence', '.', 'We', 'first', 'tokenize', 'the', 'sentence', 'into', 'words', 'using', 'nltk.word_tokenize', 'and', 'then', 'we', 'will', 'call', 'lemmatizer.lemmatize', '(', ')']\n"
     ]
    }
   ],
   "source": [
    "text = \"Let’s lemmatize a simple sentence. We first tokenize the sentence into words using nltk.word_tokenize and then we will call lemmatizer.lemmatize()\"\n",
    "word_list = nltk.word_tokenize(text)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65ba6f15-16ff-45e4-af15-3083956e3226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let ’ s lemmatize a simple sentence . We first tokenize the sentence into word using nltk.word_tokenize and then we will call lemmatizer.lemmatize ( )\n"
     ]
    }
   ],
   "source": [
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca0f44f0-0e43-40c5-99ca-d2c3303d2637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stripe'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "word = 'stripes'\n",
    "w = Word(word)\n",
    "w.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d16e627f-fbac-4a6a-b2dd-4c5db9f84956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The striped bat are hanging on their foot for best'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The striped bats are hanging on their feet for best\"\n",
    "sent = TextBlob(text)\n",
    "\" \". join([w.lemmatize() for w in sent.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "330ae0cd-cd54-406f-a81f-5613dc15f9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts of Speech:  [('The', 'DT'), ('striped', 'JJ'), ('bats', 'NNS'), ('are', 'VBP'), ('hanging', 'VBG'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('for', 'IN'), ('best', 'JJS')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "text = \"The striped bats are hanging on their feet for best\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"Parts of Speech: \",nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94217d27-c626-4c67-a57f-1497a69cd521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "documents = [\n",
    "  \"The quick brown fox jumped over the lazy dog's back\",\n",
    "  \"Now is the time for all good men to come to the aid of their party\"\n",
    "]\n",
    "vectorizer = TfidfVectorizer(stop_words=[\"for\",\"is\",\"of\",\"the\",\"to\"])\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87fcf248-ac92-42e7-8dbf-749ed70e019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix (Dense Format):\n",
      "[[0.         0.         0.35355339 0.35355339 0.         0.35355339\n",
      "  0.35355339 0.         0.35355339 0.35355339 0.         0.\n",
      "  0.35355339 0.         0.35355339 0.         0.        ]\n",
      " [0.33333333 0.33333333 0.         0.         0.33333333 0.\n",
      "  0.         0.33333333 0.         0.         0.33333333 0.33333333\n",
      "  0.         0.33333333 0.         0.33333333 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "tfidf_dense = tfidf_matrix.toarray()\n",
    "print(\"TF-IDF Matrix (Dense Format):\")\n",
    "print(tfidf_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "205c6a45-58d3-457f-835f-09edc90fb3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Names (Terms):\n",
      "['aid' 'all' 'back' 'brown' 'come' 'dog' 'fox' 'good' 'jumped' 'lazy'\n",
      " 'men' 'now' 'over' 'party' 'quick' 'their' 'time']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"\\nFeature Names (Terms):\")\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572afab0-ec80-4cc1-a079-7ce5c09a8fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
